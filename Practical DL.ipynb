{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["1) Multilayer perceptron (MLP) to solve the XOR problem"],"metadata":{"id":"MOqEBbYXTMD4"}},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","# Training data for the XOR problem\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","y = np.array([0, 1, 1, 0])\n","\n","# Define the MLP model\n","model = Sequential()\n","model.add(Dense(units=2, input_dim=2, activation='relu'))  # Hidden layer with 2 neurons\n","model.add(Dense(units=1, activation='sigmoid'))           # Output layer with 1 neuron\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X, y, epochs=10000, verbose=0)\n","\n","# Test the model\n","test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","predictions = model.predict(test_data)\n","\n","# Print the predictions\n","for i in range(len(test_data)):\n","    print(f\"Input: {test_data[i]} - Predicted Output: {predictions[i][0]}\")\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X, y)\n","print(f\"Loss: {loss}, Accuracy: {accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmOe4e4sRLPl","executionInfo":{"status":"ok","timestamp":1698765849708,"user_tz":-330,"elapsed":76647,"user":{"displayName":"Vol 1 Content","userId":"11789528331753235874"}},"outputId":"ff4eec34-2abd-4aeb-a04e-27e48eab58c0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 73ms/step\n","Input: [0 0] - Predicted Output: 0.499985933303833\n","Input: [0 1] - Predicted Output: 0.9998064637184143\n","Input: [1 0] - Predicted Output: 0.499985933303833\n","Input: [1 1] - Predicted Output: 0.00022135388280730695\n","1/1 [==============================] - 0s 141ms/step - loss: 0.3467 - accuracy: 0.7500\n","Loss: 0.346677303314209, Accuracy: 0.75\n"]}]},{"cell_type":"markdown","source":["3) To learn the parametres of the supervised  single layer feed farward neural network for Stochastic Gradient Descent"],"metadata":{"id":"FcDZBYzUTSpH"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Define the training dataset\n","X = np.array([[1], [2], [3], [4], [5]])  # Input features\n","y = np.array([[2], [3], [4], [5], [6]])  # Corresponding target values\n","\n","# Define the sigmoid activation function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Initialize random weights and bias for the single-layer neural network\n","np.random.seed(0)\n","weights = np.random.rand(1, 1)\n","bias = np.random.rand()\n","\n","# Define the learning rate and number of epochs\n","alpha = 0.01\n","epochs = 1000\n","\n","# Stochastic Gradient Descent for training the neural network\n","for epoch in range(epochs):\n","    for i in range(len(X)):\n","        # Forward propagation\n","        z = np.dot(X[i], weights) + bias\n","        output = sigmoid(z)\n","\n","        # Backward propagation\n","        error = output - y[i]\n","        d_output = error * output * (1 - output)\n","        d_weights = np.dot(X[i].T, d_output)\n","        d_bias = d_output\n","\n","        # Update weights and bias\n","        weights -= alpha * d_weights\n","        bias -= alpha * d_bias\n","\n","# Print the final weights and bias\n","print(\"Final weights:\", weights)\n","print(\"Final bias:\", bias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_h3smFtSV7M","executionInfo":{"status":"ok","timestamp":1698765858803,"user_tz":-330,"elapsed":572,"user":{"displayName":"Vol 1 Content","userId":"11789528331753235874"}},"outputId":"a2bb8330-b988-4531-c1e3-ff55a13bbcad"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Final weights: [[2.10103977]]\n","Final bias: [1.60768383]\n"]}]},{"cell_type":"markdown","source":["4) IMPLEMENTATION OF BACKWARD PROPAGATION USING DEEP NEURAL NETWORK WITH 2 HIDDEN LAYERS"],"metadata":{"id":"xHp9t1zNT9k0"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6pM6V-UQdGk","executionInfo":{"status":"ok","timestamp":1698764814088,"user_tz":-330,"elapsed":1845,"user":{"displayName":"Majid Ali","userId":"10865913695116108118"}},"outputId":"c71d6a57-588b-49be-e92f-19d420c71f3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.2619\n","Epoch 100, Loss: 0.2500\n","Epoch 200, Loss: 0.2494\n","Epoch 300, Loss: 0.2494\n","Epoch 400, Loss: 0.2493\n","Epoch 500, Loss: 0.2493\n","Epoch 600, Loss: 0.2493\n","Epoch 700, Loss: 0.2492\n","Epoch 800, Loss: 0.2492\n","Epoch 900, Loss: 0.2491\n","Epoch 1000, Loss: 0.2491\n","Epoch 1100, Loss: 0.2491\n","Epoch 1200, Loss: 0.2490\n","Epoch 1300, Loss: 0.2490\n","Epoch 1400, Loss: 0.2489\n","Epoch 1500, Loss: 0.2489\n","Epoch 1600, Loss: 0.2488\n","Epoch 1700, Loss: 0.2488\n","Epoch 1800, Loss: 0.2487\n","Epoch 1900, Loss: 0.2486\n","Epoch 2000, Loss: 0.2486\n","Epoch 2100, Loss: 0.2485\n","Epoch 2200, Loss: 0.2484\n","Epoch 2300, Loss: 0.2484\n","Epoch 2400, Loss: 0.2483\n","Epoch 2500, Loss: 0.2482\n","Epoch 2600, Loss: 0.2481\n","Epoch 2700, Loss: 0.2480\n","Epoch 2800, Loss: 0.2479\n","Epoch 2900, Loss: 0.2478\n","Epoch 3000, Loss: 0.2477\n","Epoch 3100, Loss: 0.2476\n","Epoch 3200, Loss: 0.2475\n","Epoch 3300, Loss: 0.2473\n","Epoch 3400, Loss: 0.2472\n","Epoch 3500, Loss: 0.2470\n","Epoch 3600, Loss: 0.2469\n","Epoch 3700, Loss: 0.2467\n","Epoch 3800, Loss: 0.2465\n","Epoch 3900, Loss: 0.2464\n","Epoch 4000, Loss: 0.2462\n","Epoch 4100, Loss: 0.2460\n","Epoch 4200, Loss: 0.2457\n","Epoch 4300, Loss: 0.2455\n","Epoch 4400, Loss: 0.2452\n","Epoch 4500, Loss: 0.2450\n","Epoch 4600, Loss: 0.2447\n","Epoch 4700, Loss: 0.2444\n","Epoch 4800, Loss: 0.2441\n","Epoch 4900, Loss: 0.2437\n","Epoch 5000, Loss: 0.2434\n","Epoch 5100, Loss: 0.2430\n","Epoch 5200, Loss: 0.2426\n","Epoch 5300, Loss: 0.2421\n","Epoch 5400, Loss: 0.2417\n","Epoch 5500, Loss: 0.2412\n","Epoch 5600, Loss: 0.2406\n","Epoch 5700, Loss: 0.2401\n","Epoch 5800, Loss: 0.2395\n","Epoch 5900, Loss: 0.2388\n","Epoch 6000, Loss: 0.2382\n","Epoch 6100, Loss: 0.2375\n","Epoch 6200, Loss: 0.2367\n","Epoch 6300, Loss: 0.2359\n","Epoch 6400, Loss: 0.2351\n","Epoch 6500, Loss: 0.2342\n","Epoch 6600, Loss: 0.2333\n","Epoch 6700, Loss: 0.2323\n","Epoch 6800, Loss: 0.2313\n","Epoch 6900, Loss: 0.2302\n","Epoch 7000, Loss: 0.2291\n","Epoch 7100, Loss: 0.2280\n","Epoch 7200, Loss: 0.2268\n","Epoch 7300, Loss: 0.2256\n","Epoch 7400, Loss: 0.2243\n","Epoch 7500, Loss: 0.2230\n","Epoch 7600, Loss: 0.2217\n","Epoch 7700, Loss: 0.2204\n","Epoch 7800, Loss: 0.2190\n","Epoch 7900, Loss: 0.2176\n","Epoch 8000, Loss: 0.2162\n","Epoch 8100, Loss: 0.2148\n","Epoch 8200, Loss: 0.2134\n","Epoch 8300, Loss: 0.2121\n","Epoch 8400, Loss: 0.2107\n","Epoch 8500, Loss: 0.2093\n","Epoch 8600, Loss: 0.2080\n","Epoch 8700, Loss: 0.2066\n","Epoch 8800, Loss: 0.2053\n","Epoch 8900, Loss: 0.2041\n","Epoch 9000, Loss: 0.2028\n","Epoch 9100, Loss: 0.2016\n","Epoch 9200, Loss: 0.2005\n","Epoch 9300, Loss: 0.1994\n","Epoch 9400, Loss: 0.1983\n","Epoch 9500, Loss: 0.1972\n","Epoch 9600, Loss: 0.1962\n","Epoch 9700, Loss: 0.1952\n","Epoch 9800, Loss: 0.1943\n","Epoch 9900, Loss: 0.1934\n","Predictions:\n","[[0.39403572]\n"," [0.77894436]\n"," [0.35230089]\n"," [0.38258235]]\n"]}],"source":["import numpy as np\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","class NeuralNetwork:\n","    def __init__(self, input_size, hidden_sizes, output_size):\n","        self.input_size = input_size\n","        self.hidden_sizes = hidden_sizes\n","        self.output_size = output_size\n","\n","        layer_sizes = [input_size] + hidden_sizes + [output_size]\n","        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i+1]) for i in range(len(layer_sizes) - 1)]\n","        self.biases = [np.random.randn(layer_sizes[i+1]) for i in range(len(layer_sizes) - 1)]\n","\n","    def forward_propagate(self, x):\n","        activations = [x]\n","        for i in range(len(self.weights)):\n","            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n","            a = sigmoid(z)\n","            activations.append(a)\n","        return activations\n","\n","    def backpropagate(self, X, y, learning_rate):\n","        num_samples = X.shape[0]\n","        activations = self.forward_propagate(X)\n","        deltas = [None] * len(self.weights)\n","\n","        # Calculate the error at the output layer\n","        error = y - activations[-1]\n","        deltas[-1] = error * sigmoid_derivative(activations[-1])\n","\n","        # Backpropagate the error through hidden layers\n","        for i in range(len(self.weights) - 2, -1, -1):\n","            error = deltas[i+1].dot(self.weights[i+1].T)\n","            deltas[i] = error * sigmoid_derivative(activations[i+1])\n","\n","        # Update weights and biases using gradients\n","        for i in range(len(self.weights)):\n","            self.weights[i] += learning_rate * activations[i].T.dot(deltas[i]) / num_samples\n","            self.biases[i] += learning_rate * np.mean(deltas[i], axis=0)\n","\n","    def train(self, X, y, epochs, learning_rate):\n","        for epoch in range(epochs):\n","            self.backpropagate(X, y, learning_rate)\n","            if epoch % 100 == 0:\n","                loss = np.mean((y - self.forward_propagate(X)[-1]) ** 2)\n","                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n","\n","    def predict(self, X):\n","        return self.forward_propagate(X)[-1]\n","\n","# Example usage with sample data (XOR problem)\n","input_size = 2\n","hidden_sizes = [4, 3]  # At least 2 hidden layers with 4 and 3 units respectively\n","output_size = 1\n","\n","# Sample data (XOR problem)\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","y = np.array([[0], [1], [1], [0]])\n","\n","# Create the neural network\n","nn = NeuralNetwork(input_size, hidden_sizes, output_size)\n","\n","# Train the neural network\n","nn.train(X, y, epochs=10000, learning_rate=0.1)\n","\n","# Make predictions\n","predictions = nn.predict(X)\n","print(\"Predictions:\")\n","print(predictions)"]}]}